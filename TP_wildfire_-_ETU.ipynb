{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MllEUwNmAz9q"
      },
      "source": [
        "# TP1 - IA\n",
        "Détection d'incendie à partie d'images satellites hyper-spectrales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjXzqnCAAuYu"
      },
      "source": [
        "#### Next cell must be executed only if the Notebook file has been downloaded from the github/gitlab repos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkEAJwKpWJ5R"
      },
      "source": [
        "Mount your Google Drive to Colab,\n",
        "Clone the github repository, \n",
        "Go in the github directory, \n",
        "Download and extract the Dataset (localy on the Colab virtual machine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqtwKTMIM7gq",
        "outputId": "80f863f3-32fa-4862-be32-dcfab9d7421c"
      },
      "outputs": [],
      "source": [
        "# Check if the code is running inside Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 00475c0934007c15b3298bd75314e755f504205f
        "\n",
        "import os\n",
        "\n",
        "# Install gdown for downloading files from Google Drive\n",
        "!pip install -q gdown\n",
        "\n",
        "if IN_COLAB:\n",
        "\n",
        "    # Define the path where the repo should be cloned\n",
        "    repo_path = \"/content/TP_wildfire_segmentation_ETU\"\n",
        "\n",
        "    # Clone the GitHub repository if it hasn't been cloned yet\n",
        "    if not os.path.exists(repo_path):\n",
        "        !git clone https://github.com/ThomasLOUIS1/TP_wildfire_segmentation_ETU.git {repo_path}\n",
        "    \n",
        "    %cd /content/TP_wildfire_segmentation_ETU/\n",
        "\n",
        "# Define the path to the dataset zip file inside the repo\n",
        "dataset_zip_path = \"data/dataset.zip\"\n",
        "\n",
        "# Download the dataset from Google Drive if it doesn't already exist\n",
        "if not os.path.exists(dataset_zip_path):\n",
        "    import gdown\n",
        "    gdown.download(\n",
        "        \"https://drive.google.com/uc?id=1hDrmwxIVmBtMij2h5AL9mV_v1Hs_vZYk\",  # <-- Your updated file ID\n",
        "        dataset_zip_path,\n",
        "        quiet=False\n",
        "    )\n",
        "\n",
        "# Define where the extracted dataset should be located\n",
        "dataset_check_path = \"data/RGB/\"\n",
        "\n",
        "# Extract the dataset if it hasn't been extracted yet\n",
        "if not os.path.exists(dataset_check_path):\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data\")\n",
        "\n",
        "# Define the path to the extracted data directory\n",
<<<<<<< HEAD
        "data_dir = \"data/\"\n"
=======
        "data_dir = \"data/\"\n",
        "\n",
        "\n",
        "\n"
=======
        "\n",
        "if IN_COLAB:\n",
        "    # Install gdown for downloading files from Google Drive\n",
        "    !pip install -q gdown\n",
        "    import os\n",
        "\n",
        "    # Define the path where the repo should be cloned\n",
        "    repo_path = \"/content/TP_wildfire_segmentation_ETU\"\n",
        "\n",
        "    # Clone the GitHub repository if it hasn't been cloned yet\n",
        "    if not os.path.exists(repo_path):\n",
        "        !git clone https://github.com/ThomasLOUIS1/TP_wildfire_segmentation_ETU.git {repo_path}\n",
        "\n",
        "    # Define the path to the dataset zip file inside the repo\n",
        "    dataset_zip_path = os.path.join(repo_path, \"data/dataset.zip\")\n",
        "\n",
        "    # Download the dataset from Google Drive if it doesn't already exist\n",
        "    if not os.path.exists(dataset_zip_path):\n",
        "        import gdown\n",
        "        gdown.download(\n",
        "            \"https://drive.google.com/uc?id=1hDrmwxIVmBtMij2h5AL9mV_v1Hs_vZYk\",  # <-- Your updated file ID\n",
        "            dataset_zip_path,\n",
        "            quiet=False\n",
        "        )\n",
        "\n",
        "    # Define where the extracted dataset should be located\n",
        "    dataset_extract_path = \"/content/TP_wildfire_segmentation_ETU/data/RGB\"\n",
        "\n",
        "    # Extract the dataset if it hasn't been extracted yet\n",
        "    if not os.path.exists(dataset_extract_path):\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content/TP_wildfire_segmentation_ETU/data\")\n",
        "\n",
        "    # Define the path to the extracted data directory\n",
        "    data_dir = \"/content/TP_wildfire_segmentation_ETU/data\"\n",
        "\n",
        "    %cd /content/TP_wildfire_segmentation_ETU/"
>>>>>>> c4e6f9f626e667c241c3e0ef0c601d17b65190b6
>>>>>>> 00475c0934007c15b3298bd75314e755f504205f
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW968eRhAuY0"
      },
      "source": [
        "# Wildfire detection (Segmentation) with Multi-spectral Images"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What is Image Segmentation ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Image Segmentation is the process of dividing an image into multiple segments or regions, each of which corresponds to a different object or part of the image.\n",
        "\n",
        "For example, in fire segmentation, the goal is to identify and segment the regions of fire in a video or an image. The segmented fire pixels can then be used to detect the extent and spread of the fire. This information can be useful for fire-fighters to respond to emergency situations more effectively."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WYBjEuzqAuY1"
      },
      "source": [
        "### What are multi-spectrale Images and why using it for Wildfire Segmentation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awizkYyLAuY1"
      },
      "source": [
        "Multi-spectral images are images representing the same area in multiple wavelength band. They are very useful for wildfire segmentation because they can provide more information about the characteristics of the wildfire and the surrounding area than RGB images. \n",
        "\n",
        "For example, near-infrared (NIR) bands can be used to identify vegetation, while shortwave infrared (SWIR) bands can be used to detect the presence of smoke. Additionally, using multiple wavelength bands can help to reduce the impact of atmospheric conditions such as clouds and haze on the image. This can improve the accuracy of the segmentation task and make it easier to identify a wildfire in the image.\n",
        "\n",
        "These images are stored in `.tif files`. TIFF is a widely-used file format for images. It is capable of storing images in a lossless format, meaning that no data is lost when the image is compressed. This makes it a popular choice for storing high-quality images, such as those used in professional photography, printing or satellite imagery. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LLEE9GnWYAG"
      },
      "source": [
        "# 0.1 Get dataset files paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNYM-7OaAuY3"
      },
      "source": [
        "Loading a segmentation dataset differ from loading a classification dataset with a tensorflow or Keras function. When using Keras for MNIST for example, images are stored in folder correspondign to a class. In folder 0, there are all images for the number 0.\n",
        "\n",
        "Here, we will not load data from classes since data do not belong to any classes. The segmentation task will generate a mask (having the same size as the input image) and each pixel of the mask is a value 0 or 1 corresponding to the fire/notfire class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moaMvd0uQRWW",
        "outputId": "b25f099c-9080-4d41-e9df-ab82a71d3667"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "# Dataset folder paths declaration\n",
        "RGB_dir = data_dir + \"/______\"                    # Only for display purpose\n",
        "triband_dir = data_dir + \"/______\"    # Data directory\n",
        "mask_dir = data_dir + \"/______\"          # Labels directory\n",
        "\n",
        "# Define images size\n",
        "img_size = (______, ______)\n",
        "\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "######################################\n",
        "# What's the purpose of the following function ?\n",
        "# Answer : ___________________________________.\n",
        "def sort_tif_paths_from_folder(dir):\n",
        "    \"\"\"\n",
        "    Get all tif files sorted by name in a directory and return as a sorted list.\n",
        "    \n",
        "    Parameters:\n",
        "    dir (str): directory path containing the tif files\n",
        "\n",
        "    Returns:\n",
        "    list: sorted list of tif file paths in the directory\n",
        "    \"\"\"\n",
        "    paths_list = sorted(\n",
        "    [\n",
        "        os.path.join(dir, fname)\n",
        "        for fname in os.listdir(dir)\n",
        "        if fname.endswith(\".tif\")\n",
        "    ]\n",
        "    )\n",
        "    return paths_list\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "# Get sorted list of tif files for RGB images\n",
        "RGB_img_paths = sort_tif_paths_from_folder(RGB_dir)\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "# Get sorted list of tif files for triband training images\n",
        "triband_img_paths_train = sort_tif_paths_from_folder(triband_dir + \"/______\" )\n",
        "# Get sorted list of tif files for triband validation images\n",
        "triband_img_paths_val = sort_tif_paths_from_folder(triband_dir + \"/______\")\n",
        "# Get sorted list of tif files for triband test images\n",
        "triband_img_paths_test = sort_tif_paths_from_folder(triband_dir + \"/______\")\n",
        "\n",
        "# Get sorted list of tif files for mask training images\n",
        "mask_img_paths_train = sort_tif_paths_from_folder(mask_dir + \"/______\")\n",
        "# Get sorted list of tif files for mask validation images\n",
        "mask_img_paths_val = sort_tif_paths_from_folder(mask_dir + \"/______\")\n",
        "# Get sorted list of tif files for mask test images\n",
        "mask_img_paths_test = sort_tif_paths_from_folder(mask_dir + \"/______\")\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "# The idea is to compute the number of samples we have. \n",
        "# Tips : you have to sum the length of triband_img_paths_train, triband_img_paths_val and triband_img_paths_test arrays\n",
        "######################################\n",
        "print(\"Number of samples from {} : {}\".format(triband_dir, ______))\n",
        "print(\"Number of samples from {} : {}\".format(mask_dir, ______))\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "# Print 6 firsts paths from RGB, biband and mask paths\n",
        "# Note: Only works for the first 15 because we don't have many RGB images\n",
        "for RGB_path, triband_path, mask_path in zip(RGB_img_paths[:6], triband_img_paths_train[:6], mask_img_paths_train[:6]):\n",
        "    print(RGB_path, \"|\", triband_path, \"|\", mask_path)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0d6ESBhIX8u3"
      },
      "source": [
        "# 0.2 Let us visualize RGB / Tri-bands / mask datas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SU9SFiLwc77e"
      },
      "source": [
        "First let us import a function to display several images (declared in /utils.py) and others utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTUwJNd8b4M5"
      },
      "outputs": [],
      "source": [
        "# Display 1 or more numpy matrix \n",
        "from utils import display_matrix\n",
        "\n",
        "# Used to load/Read/ and plot images \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Used to perform manipulation on matrix/images\n",
        "import numpy as np\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Visualization "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EiFO_Pj5dNv-"
      },
      "source": [
        "Let us visualize RBG and tri-bands image as well as the ground thruth mask.\n",
        "\n",
        "You will see that with the RGB image, it's very difficult to see the fire... But with the tri-bands image, it's much more easier !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HjOFgzqCYC39",
        "outputId": "a51ab9d1-8270-4492-a5d1-8ae7dcd31e4e"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "n_images_to_display = 3\n",
        "for id in range(len(RGB_img_paths)):\n",
        "    # Read the mask data for the current iteration\n",
        "    image_masks = Image.open(mask_img_paths_train[id])\n",
        "    # Check if there's at least one \"Fire\" pixel and if the number of images displayed is less than 3\n",
        "\n",
        "    if np.max(image_masks) > 0 and i < n_images_to_display:\n",
        "        # Read RGB and Tri-bands data\n",
        "        image_RGB = Image.open(RGB_img_paths[id])\n",
        "        image_triband = Image.open(triband_img_paths_train[id])\n",
        "\n",
        "        # print id and path of the associated RGB path\n",
        "        print(\"Image n {}, path : {}\".format(id, RGB_img_paths[id]))\n",
        "        \n",
        "        # Store data in matrix and display it\n",
        "        matrix = [[image_RGB, image_triband, image_masks]]\n",
        "        display_matrix(matrix, title_list=[\"RGB\", \"Tri-Bands\", \"Ground Truth Mask\"])\n",
        "        \n",
        "        # Increment counter for number of images displayed\n",
        "        i += 1\n",
        "\n",
        "    # Break out of loop if 3 images have been displayed\n",
        "    if i == n_images_to_display:\n",
        "        break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\">What do you think of Tribands images compared to RGB images ?</font>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Put your answer bellow** : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pbbQWurXXJCN"
      },
      "source": [
        "# 0.3 Load dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now load the dataset to use it later for training.\n",
        "\n",
        "First we define a function to create a dataset from paths gathered previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "######################################\n",
        "# Complete the following code replacing \"______\" : \n",
        "######################################\n",
        "def load_triband_and_mask_from_paths(paths = None):\n",
        "    \"\"\"\n",
        "    Load the tri-bands and mask images data from the given paths and convert them into a tensorflow dataset.\n",
        "    \n",
        "    Parameters:\n",
        "    paths (List): List of tuples of tri-bands and mask image paths\n",
        "    \n",
        "    Returns:\n",
        "    data (tf.data.Dataset): A tensorflow dataset object with tri-bands and mask images data.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize arrays to store tri-bands image data and mask image data\n",
        "    x = np.zeros((len(paths),) + img_size + (3,), dtype=\"float32\")\n",
        "    y = np.zeros((len(paths),) + img_size + (1,), dtype=\"float32\")\n",
        "    \n",
        "    # Loop through the list of tri-bands and mask image paths\n",
        "    for i, (triband_path, mask_path)  in enumerate(paths):\n",
        "        \n",
        "        # Read the tri-bands image file and normalize the data\n",
        "        triband = np.array(Image.open(______)) / ______\n",
        "        x[i] = triband\n",
        "        \n",
        "        # Open the mask image file\n",
        "        mask = np.array(Image.open(______))\n",
        "        # Add an extra dimension to the mask data for compatibility with the model\n",
        "        mask = np.expand_dims(mask, 2) # same as np.reshape(mask, (256,256,1))\n",
        "        y[i] = mask\n",
        "        \n",
        "    # Create a tensorflow dataset from tri-bands and mask image data\n",
        "    data = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    \n",
        "    # Batch the dataset and fetch the data in advance for faster processing\n",
        "    data = data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return data\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we use it to load train, val and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create tuples of (Tri-bands image path, mask image path) for train, val and test datasets\n",
        "train_paths = list(zip(triband_img_paths_train, mask_img_paths_train))\n",
        "val_paths = list(zip(triband_img_paths_val, mask_img_paths_val))\n",
        "test_paths = list(zip(triband_img_paths_test, mask_img_paths_test))\n",
        "\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "# Load train, val and test datasets from the tuple of Tri-bands and mask image paths\n",
        "train_ds =  load_triband_and_mask_from_paths(______)\n",
        "val_ds = load_triband_and_mask_from_paths(______)\n",
        "test_ds = load_triband_and_mask_from_paths(______)\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.0 Let us Build the CNN model for image segmentation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will create a very classical [Unet-like CNN model](https://medium.com/analytics-vidhya/what-is-unet-157314c87634) that we'll be used for each training."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\">Create the Unet-like CNN model with the following architecture : </font>\n",
        "1. A **2D convolution** (`Conv2D`) with 8 filters of 3*3 and a ReLU activation \n",
        "2. A **Max Pooling 2D** with 2*2 kernel \n",
        "---\n",
        "3. A **2D convolution** with 4 filters of 3*3 and a ReLU activation \n",
        "---\n",
        "4. A **Dropout** layer at 25%\n",
        "---\n",
        "5. A **2D Transpose Convolution** (`Conv2DTranspose`) with 4 filters of 3*3 and a ReLU activation \n",
        "6. A **Up Sampling 2D** (`UpSampling2D`) with 2*2 kernel \n",
        "---\n",
        "7. A **2D Transpose Convolution** with 8 filters of 3*3 and a ReLU activation \n",
        "---\n",
        "8. A **2D Convolution** with 1 filter of 3*3, a Sigmoid activation with a same padding\n",
        "\n",
        "At the end display the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the code with the architecture of the model described above : \n",
        "######################################\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(___________)\n",
        "\n",
        "...\n",
        "\n",
        "model.add(___________)\n",
        "\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kD9QgU1ueqHJ"
      },
      "source": [
        "# 1.1 Clone the 1st model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us clone this model into a new \"model1\". By doing that, we will be able to perform different trainings with the same starting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJId_sdsess9",
        "outputId": "2c553584-7e21-4a3c-fa89-0cb6a3ef691b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "model1 = clone_model(model)\n",
        "\n",
        "model1.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\"> Compile the model with the following parameters : </font>\n",
        " - An Adam optimizer with a learning rate at 0.01\n",
        " - A Binary Accuracy metrics\n",
        " - A Binary Crossentropy loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add Optimizer, Metric(s), Loss(es) and compile the model1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "model1.compile(optimizer = ______, metrics = ______, loss = ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aQmfpy02fWlf"
      },
      "source": [
        "# 1.2 Train the model1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will train the model1 for 4 epochs with \"train_ds\" dataset for trainong and \"val_ds\" for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caupMupdfDxU",
        "outputId": "8d3e90cd-fded-44fb-e46e-bf3c86265f56"
      },
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "history = model1.fit(______, epochs = ______, validation_data = ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us evaluate the model1 with the test dataset (test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import a function that display scores and values (see utils.py)\n",
        "from utils import print_score\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "model1_score = ______.evaluate(______)\n",
        "\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "# Display model1 scores\n",
        "print_score(model1_score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\">What do you think of the model1 scores ?</font>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Put your answer bellow** : "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# But, is this score really accurate ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check this score with diplaying some prediction from the test set..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import a predict function similar to model.predict but with a treshhold at 0.5 (see utils.py)\n",
        "from utils import predict\n",
        "\n",
        "# A function that display firsts n_range tri-bands, masks and predictions\n",
        "def display_sample_prediction(model, triband_paths, mask_paths, n_range = 3): \n",
        "  # Pass through n_range firsts images \n",
        "  i = 0\n",
        "  for id in range(len(triband_paths)):\n",
        "    # Read the mask data for the current iteration\n",
        "    image_masks = Image.open(mask_paths[id])\n",
        "    # Check if there's at least one \"Fire\" pixel and if the number of images displayed is less than n_range\n",
        "\n",
        "    if np.max(image_masks) > 0 and i < n_range:\n",
        "          \n",
        "      # Read and normalize triband\n",
        "      n_triband = np.array(Image.open(triband_paths[id]))/255.0\n",
        "      \n",
        "      # Add an extra dimension to the triband data for compatibility with the model input\n",
        "      tribands = np.expand_dims(n_triband, 0) # Same as np.reshape(tribands, (1,256,256,3))\n",
        "\n",
        "      # print id and path of the associated Tri-bands path\n",
        "      print(\"Image n {}, path : {}\".format(id, triband_paths[id]))\n",
        "      \n",
        "      predicted_mask = predict(model, tribands)\n",
        "      predicted_mask = np.squeeze(predicted_mask)\n",
        "\n",
        "      matrix=[[n_triband, image_masks, predicted_mask]] # [[Tribands,mask,predict]]\n",
        "\n",
        "      display_matrix(matrix ,title_list=['Tri-bands', 'GT Mask', 'Predicted'])\n",
        "      \n",
        "      # Increment counter for number of images displayed\n",
        "      i += 1\n",
        "    \n",
        "    # Break out of loop if n_range images have been displayed\n",
        "    if i == n_range:\n",
        "        break\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "# Diplay Tri-bands, mask and prediction of the model1\n",
        "display_sample_prediction(______, ______, ______)\n",
        "\n",
        "######################################\n",
        "######################################\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\">What do you think of the Predictions compared to the Masks ?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JG8TTBkAuZI"
      },
      "source": [
        "**Put your answer bellow** : "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The effect of an unbalanced dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The problem comes from the fact that there are more \"No_fire\" pixels than \"Fire\" pixels, much much more. \n",
        "\n",
        "As a consequence, the neural network neglicts these \"Fire\" pixels, and will classify all pixels as \"No_fire\"... \n",
        "\n",
        "We can see it with a Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import display_confusion_matrix, load_masks\n",
        "\n",
        "test_gt_masks = load_masks(mask_img_paths_test, img_size=img_size)\n",
        "\n",
        "display_confusion_matrix(predict(model1, test_ds), test_gt_masks)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\">Compute Precision and Recall ?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JG8TTBkAuZI"
      },
      "source": [
        "**Put your answer bellow** : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyCN3ZamAuZK"
      },
      "source": [
        "# 1.3 Dataset balancing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXBdROy3AuZK"
      },
      "source": [
        "There are several ways to prevent bad generalization from an unbalanced dataset such as :\n",
        "\n",
        " - Resampling techniques such as oversampling the minority class or undersampling the majority class to balance the dataset.\n",
        " - Collecting more data to balance the classes or using a different dataset.\n",
        "\n",
        "In our case, the simpliest way is to use a **weighted loss function** to give more importance to the minority class (here Fire) during training.\n",
        "\n",
        "On the top of that we will also use more metrics to see if the \"Fire\" class is properly classified."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kD9QgU1ueqHJ"
      },
      "source": [
        "# 2.1 Clone and train the 2nd model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us clone the same Unet-like CNN model to test the weighted loss function...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJId_sdsess9",
        "outputId": "2c553584-7e21-4a3c-fa89-0cb6a3ef691b"
      },
      "outputs": [],
      "source": [
        "model2 = clone_model(model)\n",
        "\n",
        "model2.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.2 Train with weighted loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time we will use the **weighted binary crossentropy loss** function defined below.\n",
        "\n",
        "First, we need to choose a weight for each class \"Fire\" and \"No_fire\". To do so we have 3 choices :\n",
        " - Try iteratively several weights ( Could be very long )\n",
        " - Use backprop to compute these parameters ( Could be difficult )\n",
        " - Compute the number of \"Fire\" pixels and \"No_Fire\" pixels to get a proportion of each pixels\n",
        "\n",
        "Here we will try to compute the number of \"Fire\" pixels and \"No_Fire\" pixels in our train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zeros = 0\n",
        "ones  = 0\n",
        "for x, y in train_ds:\n",
        "  zeros += np.count_nonzero(y == 0)  # No_Fire pixel\n",
        "  ones += np.count_nonzero(y == 1)   # Fire pixel\n",
        "\n",
        "ones_proportion = ones/(min(ones,zeros))\n",
        "zeros_proportion = zeros/(min(ones,zeros))\n",
        "\n",
        "ones_weight = zeros_proportion\n",
        "zeros_weight = ones_proportion\n",
        "\n",
        "print(\"Number of ones : {} | Number of zeros : {}\".format(ones, zeros))\n",
        "print(\"Ones proportion : {} | Zeros proportion : {}\".format(ones_proportion, zeros_proportion))\n",
        "print(\"Ones weight : {} | Zeros weight : {}\".format(ones_weight, zeros_weight))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These results means that for 1 \"Fire\" pixel, we have around 550 \"No_Fire\" pixels. Meaning our dataset is clearly unbalanced.\n",
        "\n",
        "We could use this proportion of \"No_Fire\" pixel (550) as a class weight for \"Fire\" class, but this is a too big number to use it for weighted binary crossentropy loss computation. In fact, if we try this in our model, training will not converge... The reason is that if a batch contains a large amount of \"Fire\" pixels, the loss will \"explode\" and the training will diverge. One way to prevent this could be to reduce the learning rate, thus the loss. \n",
        "\n",
        "In the case of strong unbalanced classes, finding the best weight class has to be performed in an iterative way.\n",
        "\n",
        "Using this iterative process, a class weight of 26 for Fire class has shown good results. In another notebook you will be able to try different class weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ones_weight = 26     # Fire class\n",
        "zeros_weight = 1.0   # No_Fire class"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a custom loss function to put weight for each class :\n",
        "+ To create a custom loss, the idea is to declare a function **weighted_binary_crossentropy** that takes two inputs, **y_true** and **y_pred**, which represent the true label and the predicted label respectively. \n",
        "+ The **y_true** and **y_pred** are clipped to a small value belonging to ]0,1[ (`K.epsilon()`) to avoid numerical instability in the computation of logarithms (log(0) = -inf). \n",
        "+ The loss is computed as the weighted negative log-likelihood of the true label, where the weight is determined by two variables **ones_weight** and **zeros_weight**. The loss is then averaged across all samples (axis=-1) and returned as the final result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def weighted_binary_crossentropy( y_true, y_pred):\n",
        "    # Clipping y_true and y_pred to avoid numeric instability\n",
        "    y_true = K.clip(y_true, K.epsilon(), 1-K.epsilon())\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1-K.epsilon())\n",
        "\n",
        "    # Calculating the weighted negative log-likelihood loss\n",
        "    # The original logaritic loss is : loss = -(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
        "    loss = -(y_true * K.log(y_pred) * ones_weight + (1 - y_true) * K.log(1 - y_pred) * zeros_weight)\n",
        "\n",
        "    # Averaging the loss across all samples\n",
        "    return K.mean(loss, axis=-1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.3 Adding more metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this training we need more than the Binary Accuracy metrics as it is not enough to see if the model converges.\n",
        "\n",
        "We will use 3 metrics :\n",
        " - Recall \n",
        " - Precision \n",
        " - F1-score \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\"> Give the formula and an explanation for each metrics :</font>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Put your answer bellow** : "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Recall ...\n",
        "\n",
        " - Precision ...\n",
        "\n",
        " - F1-score  ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\"> Compile model2 with the following parameters : </font>\n",
        " - An Adam optimizer with a learning rate at 0.01\n",
        " - The weighted binary crossentropy loss created above\n",
        " - Several metrics :\n",
        "   - Binary Accuracy\n",
        "   - Recall\n",
        "   - Precision\n",
        "   - F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from metrics_and_losses import recall_m, precision_m, f1_m\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "metrics = [______, ______, ______, ______]\n",
        "\n",
        "______.compile(optimizer = ______, metrics = ______, loss = ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aQmfpy02fWlf"
      },
      "source": [
        "# 2.4 Train model2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will train the model2 for 4 epochs with \"train_ds\" dataset for trainong and \"val_ds\" for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caupMupdfDxU",
        "outputId": "8d3e90cd-fded-44fb-e46e-bf3c86265f56"
      },
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "history = model2.fit(______, epochs = ______, validation_data = ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate model2 with the test_ds dataset..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "model2_score = ______.evaluate(______)\n",
        "\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "# Display model1 scores\n",
        "print_score(model2_score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Better results ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see if the model works better using a weighted loss. We can see that by displaying the predicted mask..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "# Diplay Tri-bands, mask and prediction of the model2\n",
        "display_sample_prediction(______, ______, ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\">What do you think of the Predictions compared to the Masks ?</font>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Put your answer bellow** : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can check again the Confusion Matrix..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import display_confusion_matrix, load_masks\n",
        "\n",
        "# test_gt_masks = load_masks(mask_img_paths_test) # already load\n",
        "\n",
        "display_confusion_matrix(predict(model2, test_ds), test_gt_masks)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4VItheCvAuZI"
      },
      "source": [
        "### <font color=\"red\">Compute Precision and Recall ?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JG8TTBkAuZI"
      },
      "source": [
        "**Put your answer bellow** : "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's better, but can we improve the F1 score ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kD9QgU1ueqHJ"
      },
      "source": [
        "# 3.1 Create and train the 3rd model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us clone the same Unet-like CNN model to test a new loss function...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJId_sdsess9",
        "outputId": "2c553584-7e21-4a3c-fa89-0cb6a3ef691b"
      },
      "outputs": [],
      "source": [
        "model3 = clone_model(model)\n",
        "\n",
        "model3.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.2 Train with Dice/F-score loss\n",
        "The Dice loss is a loss function commonly used in image segmentation tasks. It is a variation of the Sørensen-Dice coefficient, which is a measure of similarity between two sets. \n",
        "\n",
        "The Dice loss calculates the difference between the predicted segmentation and the ground truth segmentation, with a range of 0 to 1, where 0 indicates no overlap and 1 indicates a perfect overlap. \n",
        "\n",
        "The Dice loss is defined as 1 - (2 * (intersection of predicted and ground truth) / (size of predicted + size of ground truth)). Lower values of the Dice loss indicate a better match between the predicted and ground truth segmentations.\n",
        "\n",
        "Bellow an implementation of the Dice loss function :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The Dice loss function we will use in the model.compile\n",
        "def dice_loss(y_true, y_pred):\n",
        "    # Flatten the y_true and y_pred tensors\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    \n",
        "    # Calculate the intersection of y_true and y_pred\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    \n",
        "    # Return the Dice loss value\n",
        "    return 1 - (2. * intersection + 1.) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\"> Compile model3 with the following parameters : </font>\n",
        " - An Adam optimizer with a learning rate at 0.01\n",
        " - The Dice loss created above\n",
        " - Several metrics :\n",
        "   - Binary Accuracy\n",
        "   - Recall\n",
        "   - Precision\n",
        "   - F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from metrics_and_losses import recall_m, precision_m, f1_m\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "metrics = [______, ______, ______, ______]\n",
        "\n",
        "______.compile(optimizer = ______, metrics = ______, loss = ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will train the model3 for 4 epochs with \"train_ds\" dataset for training and \"val_ds\" for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caupMupdfDxU",
        "outputId": "8d3e90cd-fded-44fb-e46e-bf3c86265f56"
      },
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "history = model3.fit(______, epochs = ______, validation_data = ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate model3 with the test_ds dataset..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "model3_score = ______.evaluate(______)\n",
        "\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "# Display model1 scores\n",
        "print_score(model3_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ0PJ461AuZR"
      },
      "source": [
        "## Better results ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZmPWDowAuZR"
      },
      "source": [
        "Let's see if the model works better using a Dice loss. We can see that by displaying the predicted mask..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "# Diplay Tri-bands, mask and prediction of the model3\n",
        "display_sample_prediction(______, ______, ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\">What do you think of the Predictions compared to the Masks ?</font>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Put your answer bellow** : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdDgr4JAuZb"
      },
      "source": [
        "Let us check again the Confusion Matrix..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import display_confusion_matrix, load_masks\n",
        "\n",
        "# test_gt_masks = load_masks(mask_img_paths_test) # already load\n",
        "\n",
        "display_confusion_matrix(predict(model3, test_ds), test_gt_masks)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wviHZjYjAuZb"
      },
      "source": [
        "Finaly, let's try a combination of weighted loss and dice loss."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kD9QgU1ueqHJ"
      },
      "source": [
        "# 4.1 Create and train the 4th model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FWCrObSgAuZL"
      },
      "source": [
        "Let us clone the same Unet-like CNN model to test a combination of weighted loss and dice loss...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJId_sdsess9",
        "outputId": "2c553584-7e21-4a3c-fa89-0cb6a3ef691b"
      },
      "outputs": [],
      "source": [
        "model4 = clone_model(model)\n",
        "\n",
        "model4.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iG4HcA-0AuZc"
      },
      "source": [
        "We define a loss function which combine a weighted binary crossentropy loss and a dice loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def weighted_binary_crossentropy_and_dice(y_true, y_pred):\n",
        "    return weighted_binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\"> Compile model4 with the following parameters : </font>\n",
        " - An Adam optimizer with a learning rate at 0.01\n",
        " - The loss combination of weighted binary crossentropy and dice loss created above\n",
        " - Several metrics :\n",
        "   - Binary Accuracy\n",
        "   - Recall\n",
        "   - Precision\n",
        "   - F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "metrics = [______, ______, ______, ______]\n",
        "\n",
        "______.compile(optimizer = ______, metrics = ______, loss = ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will train the model4 for 4 epochs with \"train_ds\" dataset for trainong and \"val_ds\" for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caupMupdfDxU",
        "outputId": "8d3e90cd-fded-44fb-e46e-bf3c86265f56"
      },
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "history = model4.fit(______, epochs = ______, validation_data = ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate model4 with the test_ds dataset..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "model4_score = ______.evaluate(______)\n",
        "\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "# Display model1 scores\n",
        "print_score(model4_score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And display some prediction..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "# Diplay Tri-bands, mask and prediction of the model4\n",
        "display_sample_prediction(______, ______, ______)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color=\"red\">What do you think of the Predictions compared to the Masks ?</font>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Put your answer bellow** : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can check again the Confusion Matrix..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import display_confusion_matrix, load_masks\n",
        "\n",
        "# test_gt_masks = load_masks(mask_img_paths_test) # already load\n",
        "\n",
        "display_confusion_matrix(predict(model4, test_ds), test_gt_masks)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5.1 Compare all runs !"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write a code that print each scores of each runs using the function print_score(modelX_score) :\n",
        "\n",
        "Don't forget to re-evaluate model1 with recall_m, precision_m, f1_m metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "\n",
        "metrics = [______, ______, ______, ______]\n",
        "\n",
        "______.compile(metrics = ______)\n",
        "model1_score = ______.evaluate(______)\n",
        "\n",
        "\n",
        "print(\"\\n model1_score :\")\n",
        "...\n",
        "print_score(model4_score)\n",
        "\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#6. Bonus\n",
        "\n",
        "Go to the Bonus_wildfire.ipynb Notebook.\n",
        "If you have time left, try finding the best combination of loss/weight/epochs/architecture to get a test f1_score higher than 98%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "f24e63ee82b3faf295a1597391f06534fd675e5ceceb862d046bbe914429454a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
