{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if the code is running inside Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Install gdown for downloading files from Google Drive\n",
        "!pip install -q gdown\n",
        "import os\n",
        "\n",
        "if IN_COLAB:\n",
        "\n",
        "    # Define the path where the repo should be cloned\n",
        "    repo_path = \"/content/TP_wildfire_segmentation_ETU\"\n",
        "\n",
        "    # Clone the GitHub repository if it hasn't been cloned yet\n",
        "    if not os.path.exists(repo_path):\n",
        "        !git clone https://github.com/ThomasLOUIS1/TP_wildfire_segmentation_ETU.git {repo_path}\n",
        "\n",
        "    %cd /content/TP_wildfire_segmentation_ETU/\n",
        "\n",
        "# Define the path to the dataset zip file inside the repo\n",
        "dataset_zip_path = \"data/dataset.zip\"\n",
        "\n",
        "# Download the dataset from Google Drive if it doesn't already exist\n",
        "if not os.path.exists(dataset_zip_path):\n",
        "    import gdown\n",
        "    gdown.download(\n",
        "        \"https://drive.google.com/uc?id=1hDrmwxIVmBtMij2h5AL9mV_v1Hs_vZYk\",  # <-- Your updated file ID\n",
        "        dataset_zip_path,\n",
        "        quiet=False\n",
        "    )\n",
        "\n",
        "# Define where the extracted dataset should be located\n",
        "dataset_check_path = \"data/RGB\"\n",
        "\n",
        "# Extract the dataset if it hasn't been extracted yet\n",
        "if not os.path.exists(dataset_check_path):\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data\")\n",
        "\n",
        "\n",
        "\n",
        "if IN_COLAB:\n",
        "    %cd /content/TP_wildfire_segmentation_ETU/\n",
        "    # Define the path to the extracted data directory\n",
        "    data_dir = \"/content/TP_wildfire_segmentation_ETU/data\"\n",
        "else:\n",
        "    # Define the path to the extracted data directory\n",
        "    data_dir = \"data\"\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wildfire Segmentation with Multi-spectral Images - Bonus"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Try finding the best combination of loss/weight/epochs/architecture to get a test f1_score higher than 98%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LLEE9GnWYAG"
      },
      "source": [
        "# 0.1 Get dataset files paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNYM-7OaAuY3"
      },
      "source": [
        "Loading a segmentation dataset differ from loading a classification dataset with a tensorflow or Keras function. When using Keras for MNIST for example, images are stored in folder correspondign to a class. In folder 0, there are all images for the number 0.\n",
        "\n",
        "Here, we will not load data from classes since data do not belong to any classes. The segmentation task will generate a mask (having the same size as the input image) and each pixel of the mask is a value 0 or 1 corresponding to the fire/notfire class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moaMvd0uQRWW",
        "outputId": "eed00eda-8067-41c2-d9d4-ddb4ed06bc24"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "# Dataset folder paths declaration\n",
        "RGB_dir = data_dir + \"/RGB\"                    # Only for display purpose\n",
        "triband_dir = data_dir + \"/tribands\"    # Data directory\n",
        "mask_dir = data_dir + \"/masks\"          # Labels directory\n",
        "\n",
        "# Define images size\n",
        "img_size = (256, 256)\n",
        "\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "######################################\n",
        "# What's the purpose of the following function ?\n",
        "# Answer : Get all tif files sorted by name in a directory and return as a sorted list..\n",
        "\n",
        "def sort_tif_paths_from_folder(dir):\n",
        "    \"\"\"\n",
        "    Get all tif files sorted by name in a directory and return as a sorted list.\n",
        "    \n",
        "    Parameters:\n",
        "    dir (str): directory path containing the tif files\n",
        "\n",
        "    Returns:\n",
        "    list: sorted list of tif file paths in the directory\n",
        "    \"\"\"\n",
        "    paths_list = sorted(\n",
        "    [\n",
        "        os.path.join(dir, fname)\n",
        "        for fname in os.listdir(dir)\n",
        "        if fname.endswith(\".tif\")\n",
        "    ]\n",
        "    )\n",
        "    return paths_list\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "# Get sorted list of tif files for RGB images\n",
        "RGB_img_paths = sort_tif_paths_from_folder(RGB_dir)\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "# Get sorted list of tif files for triband training images\n",
        "triband_img_paths_train =   sort_tif_paths_from_folder(triband_dir + \"/train\" )\n",
        "# Get sorted list of tif files for triband validation images\n",
        "triband_img_paths_val   =   sort_tif_paths_from_folder(triband_dir + \"/val\")\n",
        "# Get sorted list of tif files for triband test images\n",
        "triband_img_paths_test  =   sort_tif_paths_from_folder(triband_dir + \"/test\")\n",
        "\n",
        "# Get sorted list of tif files for mask training images\n",
        "mask_img_paths_train  =   sort_tif_paths_from_folder(mask_dir + \"/train\")\n",
        "# Get sorted list of tif files for mask validation images\n",
        "mask_img_paths_val    =   sort_tif_paths_from_folder(mask_dir + \"/val\")\n",
        "# Get sorted list of tif files for mask test images\n",
        "mask_img_paths_test   =   sort_tif_paths_from_folder(mask_dir + \"/test\")\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "# The idea is to compute the number of samples we have. \n",
        "# Tips : you have to sum the length of triband_img_paths_train, triband_img_paths_val and triband_img_paths_test arrays\n",
        "######################################\n",
        "print(\"Number of samples from {} : {}\".format(triband_dir, len(triband_img_paths_train)+len(triband_img_paths_val)+len(triband_img_paths_test)))\n",
        "print(\"Number of samples from {} : {}\".format(mask_dir, len(mask_img_paths_train)+len(mask_img_paths_val)+len(mask_img_paths_test)))\n",
        "######################################\n",
        "######################################\n",
        "\n",
        "# Print 6 firsts paths from RGB, biband and mask paths\n",
        "# Note: Only works for the first 15 because we don't have many RGB images\n",
        "for RGB_path, triband_path, mask_path in zip(RGB_img_paths[:6], triband_img_paths_train[:6], mask_img_paths_train[:6]):\n",
        "    print(RGB_path, \"|\", triband_path, \"|\", mask_path)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pbbQWurXXJCN"
      },
      "source": [
        "# 0.2 Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXys9ITuAuY_"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "def load_triband_and_mask_from_paths(paths = None):\n",
        "    \"\"\"\n",
        "    Load the tri-bands and mask images data from the given paths and convert them into a tensorflow dataset.\n",
        "    \n",
        "    Parameters:\n",
        "    paths (List): List of tuples of tri-bands and mask image paths\n",
        "    \n",
        "    Returns:\n",
        "    data (tf.data.Dataset): A tensorflow dataset object with tri-bands and mask images data.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize arrays to store tri-bands image data and mask image data\n",
        "    x = np.zeros((len(paths),) + img_size + (3,), dtype=\"float32\")\n",
        "    y = np.zeros((len(paths),) + img_size + (1,), dtype=\"float32\")\n",
        "    \n",
        "    # Loop through the list of tri-bands and mask image paths\n",
        "    for i, (triband_path, mask_path)  in enumerate(paths):\n",
        "        \n",
        "        # Read the tri-bands image file and normalize the data\n",
        "        triband = np.array(Image.open(triband_path)) / 255.0\n",
        "        x[i] = triband\n",
        "        \n",
        "        # Open the mask image file\n",
        "        mask = np.array(Image.open(mask_path))\n",
        "        # Add an extra dimension to the mask data for compatibility with the model\n",
        "        mask = np.expand_dims(mask, 2) # Same as np.reshape(mask, (256,256,1))\n",
        "        y[i] = mask\n",
        "        \n",
        "    # Create a tensorflow dataset from tri-bands and mask image data\n",
        "    data = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    \n",
        "    # Batch the dataset and fetch the data in advance for faster processing\n",
        "    data = data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return data\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXNy4uPHAuZA"
      },
      "outputs": [],
      "source": [
        "# Create tuples of (Tri-bands image path, mask image path) for train, val and test datasets\n",
        "train_paths = list(zip(triband_img_paths_train, mask_img_paths_train))\n",
        "val_paths = list(zip(triband_img_paths_val, mask_img_paths_val))\n",
        "test_paths = list(zip(triband_img_paths_test, mask_img_paths_test))\n",
        "\n",
        "######################################\n",
        "# Complete the folowing code replacing \"______\" : \n",
        "######################################\n",
        "# Load train, val and test datasets from the tuple of Tri-bands and mask image paths\n",
        "train_ds =  load_triband_and_mask_from_paths(train_paths)\n",
        "val_ds = load_triband_and_mask_from_paths(val_paths)\n",
        "test_ds = load_triband_and_mask_from_paths(test_paths)\n",
        "######################################\n",
        "######################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0.4 Declaration of all loss, metrics etc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from metrics_and_losses import recall_m, precision_m, f1_m\n",
        "\n",
        "from utils import predict, print_score, display_confusion_matrix, display_sample_prediction, load_masks\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, Flatten, Dense, UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "def weighted_binary_crossentropy( y_true, y_pred) :\n",
        "        y_true = K.clip(y_true, K.epsilon(), 1-K.epsilon())\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1-K.epsilon())\n",
        "        logloss = -(y_true * K.log(y_pred) * ones_weight + (1 - y_true) * K.log(1 - y_pred) * zeros_weight )\n",
        "        return K.mean( logloss, axis=-1)\n",
        "\n",
        "# The Dice loss function we will use in the model.compile\n",
        "def dice_loss(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection + 1.) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.)\n",
        "\n",
        "def weighted_binary_crossentropy_and_dice(y_true, y_pred):\n",
        "    return weighted_binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kD9QgU1ueqHJ"
      },
      "source": [
        "# 1.1 Build the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here you will find a very basic CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJId_sdsess9",
        "outputId": "2c553584-7e21-4a3c-fa89-0cb6a3ef691b"
      },
      "outputs": [],
      "source": [
        "model = None\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(4, (3, 3), activation='relu', padding='same', input_shape=(256, 256, 3)))\n",
        "model.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aQmfpy02fWlf"
      },
      "source": [
        "# 1.2 Train the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose your hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "####\n",
        "# If weighted_binary_crossentropy used\n",
        "ones_weight = 26.0\n",
        "zeros_weight = 1.0\n",
        "####\n",
        "loss = weighted_binary_crossentropy\n",
        "learning_rate=0.01\n",
        "batch_size = 16\n",
        "epochs = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caupMupdfDxU",
        "outputId": "8d3e90cd-fded-44fb-e46e-bf3c86265f56"
      },
      "outputs": [],
      "source": [
        "metrics = [tf.keras.metrics.BinaryAccuracy(), recall_m, precision_m, f1_m]\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "\n",
        "callback = EarlyStopping(monitor=f1_m, mode = 'max', patience=5)\n",
        "\n",
        "model.compile(optimizer = Adam(learning_rate=learning_rate), metrics = metrics, loss = loss)\n",
        "\n",
        "history = model.fit(train_ds, epochs = epochs, validation_data = val_ds, batch_size=batch_size, callbacks = [callback])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.3 Evaluate the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_score = model.evaluate(test_ds)\n",
        "print_score(model_score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.4 Display predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pwNfTgkvAuZR",
        "outputId": "672e2c86-4cc6-4172-bf49-fd4d8b546300"
      },
      "outputs": [],
      "source": [
        "# Diplay Tri-bands, mask and prediction of the model2\n",
        "display_sample_prediction(model, triband_img_paths_test, mask_img_paths_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6iLfo1IBAuZJ"
      },
      "source": [
        "We can see it with a Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "GuWsC9nrAuZJ",
        "outputId": "2fd1518b-f17c-4241-e3d3-03599dc60aa4"
      },
      "outputs": [],
      "source": [
        "test_gt_masks = load_masks(mask_img_paths_test, img_size=img_size)\n",
        "\n",
        "display_confusion_matrix(predict(model, test_ds), test_gt_masks)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print scores again ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_score(model_score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.5 Track your tests if you want !  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| test n°       |example| 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     | 10    | ...   | \n",
        "| ---           | ---   | ---   | ---   | ---   | ---   | ---   | ---   | ---   | ---   | ---   | ---   | ---   |\n",
        "| f1_score      | 0.83  | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |\n",
        "| recall        | 0.97  | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |\n",
        "| precision     | 0.72  | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |\n",
        "| epochs        | 4     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |\n",
        "| batch_size    | 16    | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |\n",
        "| learning rate | 0.01  | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |\n",
        "| loss          | Dice  | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |\n",
        "| class weight  | None  | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |\n",
        "| n conv2d      | 3     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |\n",
        "| n conv2dT     | 2     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |\n",
        "| n params      | 1033  | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     | ?     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a1122f5ae7c14e91f12b51694de35a8e28d45bdaa122030b4c69128f2f9f3b95"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
